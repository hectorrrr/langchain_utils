{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:white; font-size:180%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > TABLE OF CONTENTS<br><div>\n",
    "* [IMPORTS](#1)\n",
    "* [Introduction](#2)\n",
    "* [GraphQA Chain](#3)\n",
    "* [Custom Chain](#4)\n",
    "* [Semantic Retrieval](#5)\n",
    "* [Adding Memory](#6)\n",
    "* [Final Chain](#7)\n",
    "* [Next Steps](#8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import langchain\n",
    "## Chains\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "from langchain_community.chains.graph_qa.prompts import (\n",
    "    CYPHER_QA_PROMPT,\n",
    "    CYPHER_GENERATION_PROMPT\n",
    ")\n",
    "\n",
    "# Prompts:\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "\n",
    "## LLMs:\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Memory\n",
    "## Memory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Introduction<br><div>\n",
    "\n",
    "In this notebook we are going to show how to create a 'Custom' GraphRAG set up, using as an example the GraphQAChain client provided by Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > GraphQA Chain<br><div>\n",
    "\n",
    "We have based this development in the [GraphCypherQAChain](https://api.python.langchain.com/en/latest/chains/langchain_community.chains.graph_qa.cypher.GraphCypherQAChain.html) chain. We are going to show how to replicate it's main behaviour here, but adapting it to the LCEL langchain notation, considering that those old chains are deprecated. In this way we ensure that our final solution will be more 'production ready', and also will be more customized.\n",
    "\n",
    "\n",
    "Inspecting the chain class definition we realized that, by default, it uses two predefined prompts:\n",
    "\n",
    "* Query generation prompt: Handles the conversion from a user query to a CYPHER query\n",
    "\n",
    "* QuestionAnswer prompt: Once the context has been retrieved from our Knowledge graph, this prompt handles the conversation\n",
    "\n",
    "Let's take a look to this prompts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question', 'schema'], template='Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CYPHER_GENERATION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\nThis will be the schema of the graph\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\nHow to build a confusion matrix with plotly?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CYPHER_GENERATION_PROMPT.invoke({'question':\"How to build a confusion matrix with plotly?\",'schema':'This will be the schema of the graph'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nHere is an example:\\n\\nQuestion: Which managers own Neo4j stocks?\\nContext:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\\nHelpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\\n\\nFollow this example when generating answers.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CYPHER_QA_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this second prompt is just to handle the conversation, once the query has retrieved some content, so we will focus in the first one.\n",
    "\n",
    "<a id=\"4\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Custom Chain<br><div>\n",
    "\n",
    "We are going to replicate here a chain that has mainly the same behaviour, but adapted to our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\") # gpt-4-0125-preview occasionally has issues\n",
    "\n",
    "graph_cypher_chain = CYPHER_GENERATION_PROMPT | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph_cypher_chain.invoke({'question':\"How to build a confusion matrix with plotly?\",'schema':'This will be the schema of the graph'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATCH (a:Actual)-[r:PREDICTED_AS]->(p:Predicted)\\nWITH {label: a.label, prediction: p.label} as data, count(*) as count\\nRETURN data, count'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this value makes not sense at all, because we have not provided the schema to the LLM yet, let's reproduce this part based in the reference Chain\n",
    "\n",
    "### Load a graph from neo4j\n",
    "\n",
    "We do this with the wrapper that langchain community offers. We could create our own, but for now we will just stick to it.\n",
    "The main advantage of this client is that directly create an schema for us, so we can contextualize the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.environ['NEO4J_PASSWORD'],database='graphrag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function allows us to directly get the schema from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_props': {'Function': [{'property': 'description', 'type': 'STRING'},\n",
       "   {'property': 'embedding', 'type': 'LIST'},\n",
       "   {'property': 'name', 'type': 'STRING'},\n",
       "   {'property': 'code', 'type': 'STRING'},\n",
       "   {'property': 'file_path', 'type': 'STRING'}],\n",
       "  'Area': [{'property': 'name', 'type': 'STRING'}],\n",
       "  'SubArea': [{'property': 'name', 'type': 'STRING'}],\n",
       "  'Framework': [{'property': 'name', 'type': 'STRING'}],\n",
       "  'Class': [{'property': 'description', 'type': 'STRING'},\n",
       "   {'property': 'name', 'type': 'STRING'},\n",
       "   {'property': 'code', 'type': 'STRING'},\n",
       "   {'property': 'file_path', 'type': 'STRING'}]},\n",
       " 'rel_props': {},\n",
       " 'relationships': [{'start': 'Area',\n",
       "   'type': 'CONTAINS_SUBAREA',\n",
       "   'end': 'SubArea'},\n",
       "  {'start': 'Area', 'type': 'CONTAINS_FRAMEWORK', 'end': 'Framework'},\n",
       "  {'start': 'SubArea', 'type': 'CONTAINS_FRAMEWORK', 'end': 'Framework'},\n",
       "  {'start': 'Framework', 'type': 'CONTAINS_FUNCTION', 'end': 'Function'},\n",
       "  {'start': 'Framework', 'type': 'CONTAINS_CLASS', 'end': 'Class'}],\n",
       " 'metadata': {'constraint': [], 'index': []}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_structured_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the function with this schema as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph_cypher_chain.invoke({'question':\"How to use plotly framework?\",'schema':graph.get_schema})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework {name: \"plotly\"})\\nRETURN f'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other advantage of the langchain graph client is that allows to directly run the queries returned by this first LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = graph.query(result.content)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'f': {'name': 'plotly'}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we added the keyword 'framework' in the question, but that is hightly unlikely in a normal query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MATCH (a:Area)-[:CONTAINS_SUBAREA]->(sa:SubArea)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\\nWHERE func.name = 'plotly'\\nRETURN a, sa, f, func;\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph_cypher_chain.invoke({'question':\"How to use plotly?\",'schema':graph.get_schema})\n",
    "\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = graph.query(result.content)[:5]\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first problem that we see is that considering how our graph is built, the entities are difficult to assign only by their name. This is a generic problem of this kind of solution. Entities should have a very descriptive name (Person, Organization...) so they can be eassily identified by a general LLM. So we should try and contextualize better about the entities that the LLM can expect. For that we will take as reference the base prompt used in the GraphCypherChain and add the following lines defining the entities for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a helpful assistant that understands the context of data science and can generate Cypher queries to retrieve information from a Neo4j database.\n",
    "\n",
    "The database schema includes the following entities:\n",
    "- Data Preprocessing Area: Nodes labeled as 'DataPreprocessingArea' representing areas of data preprocessing.\n",
    "- SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing.\n",
    "- Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\n",
    "- Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\n",
    "- Function: Nodes labeled as 'Function' representing specific functions within frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cypher_gen_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a Cypher language expert.\n",
    "    Your Task:Generate Cypher statement to query a graph database.\n",
    "    To better contextualize, the Graph database is mapping the Data Science implementations using python\n",
    "    and is divided in the following entities:\n",
    "\n",
    "    Instructions:\n",
    "    Use only the provided relationship types and properties in the schema.\n",
    "    Do not use any other relationship types or properties that are not provided.\n",
    "    Schema:\n",
    "    {schema}\n",
    "    Note: Do not include any explanations or apologies in your responses.\n",
    "    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "    Do not include any text except the generated Cypher statement.\n",
    "        - Data Preprocessing Area: Nodes labeled as 'Area' representing areas of Data Science, like 'Data Visualization' or 'Data Preprocessing'.\n",
    "        - SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing. This field is optional, some of the nodes may not have a relation with a 'SubArea' node, so generally \n",
    "        should not be added in the query.\n",
    "        - Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\n",
    "        - Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\n",
    "        - Function: Nodes labeled as 'Function' representing custom functions built on top of those frameworks.\n",
    "    Nodes do not neccesarily have parents of each type of label.\n",
    "\n",
    "    Your main focus should be to identify the Framework and the Function that is being asked.\n",
    "    The question is:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\") # gpt-4-0125-preview occasionally has issues\n",
    "\n",
    "custom_prompt_chain = cypher_gen_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = custom_prompt_chain.invoke({'question':\"How to use plotly?\",'schema':graph.get_schema})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MATCH (:Framework{name: 'plotly'})-[:CONTAINS_FUNCTION]->(f:Function)\\nRETURN f;\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATCH (:Framework{name: \"plotly\"})-[:CONTAINS_FUNCTION]->(:Function{name: \"generate_confusion_matrix\"})\\nRETURN *;'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result = custom_prompt_chain.invoke({'question':\"How to use plotly to generate a confusion matrix?\",'schema':graph.get_schema})\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Generated Cypher Statement is not valid\n{code: Neo.ClientError.Statement.SyntaxError} {message: RETURN * is not allowed when there are no variables in scope (line 2, column 1 (offset: 104))\r\n\"RETURN *;\"\r\n ^}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCypherSyntaxError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:419\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:314\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_disabled_classifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_result\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:221\u001b[0m, in \u001b[0;36mResult._run\u001b[1;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:409\u001b[0m, in \u001b[0;36mResult._attach\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:860\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    857\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    858\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    859\u001b[0m )\n\u001b[1;32m--> 860\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:370\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[1;31mCypherSyntaxError\u001b[0m: {code: Neo.ClientError.Statement.SyntaxError} {message: RETURN * is not allowed when there are no variables in scope (line 2, column 1 (offset: 104))\r\n\"RETURN *;\"\r\n ^}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      2\u001b[0m context\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:425\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_data\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CypherSyntaxError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Cypher Statement is not valid\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Generated Cypher Statement is not valid\n{code: Neo.ClientError.Statement.SyntaxError} {message: RETURN * is not allowed when there are no variables in scope (line 2, column 1 (offset: 104))\r\n\"RETURN *;\"\r\n ^}"
     ]
    }
   ],
   "source": [
    "context = graph.query(result.content)[:5]\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\\nWHERE f.name = 'plotly' AND func.name = 'plot_confusion_matrix'\\nRETURN func.code\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 477, 'total_tokens': 525}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-06f7b36a-04ba-4b7c-a7da-27ba12d48e27-0', usage_metadata={'input_tokens': 477, 'output_tokens': 48, 'total_tokens': 525})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result = custom_prompt_chain.invoke({'question':\"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",'schema':graph.get_schema})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'func.code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = graph.query(result.content)[:5]\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that after contextualizing what can be understood as a 'Framework' in our graph, the LLM is correctly identifying plotly as a framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_template =  PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant that helps to form nice and human understandable answers.\n",
    "    The information part contains the provided information that you must use to construct an answer.\n",
    "    The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
    "    Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
    "    If the provided information is empty, say that you don't know the answer.\n",
    "    Information:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sligthly adapted the prompt from the reference chain to our end, removing the example mainly. Let's build the complete chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cypher_query(query):\n",
    "    try:\n",
    "        print(\"Generated query---->\",query.content)\n",
    "        node_contents = graph.query(query.content)[:5]\n",
    "        return node_contents\n",
    "    except: \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_prompt_template' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m full_qa_chain \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: cypher_gen_prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m RunnableLambda(run_cypher_query), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: RunnablePassthrough()} \u001b[38;5;241m|\u001b[39m \u001b[43mqa_prompt_template\u001b[49m \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m      3\u001b[0m langchain\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m full_qa_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to use plotly to generate a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot_confusion_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m function? Provide the code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m:graph\u001b[38;5;241m.\u001b[39mget_schema})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_prompt_template' is not defined"
     ]
    }
   ],
   "source": [
    "full_qa_chain = {'context': cypher_gen_prompt | llm | RunnableLambda(run_cypher_query), 'question': RunnablePassthrough()} | qa_prompt_template | llm\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "full_qa_chain.invoke({'question':\"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",'schema':graph.get_schema})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was everything regarding the usage of the Graph based in a query retrieval strategy. But seing that this is not always accurate and may not give any result, we are going to mix it with a 'Semantic simmilarity' retrieval procedure.\n",
    "\n",
    "<a id=\"5\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Semantic retrieval<br><div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to define again an instance of the vector index we created in the [Neo4j Vector Store Tutorial](https://github.com/hectorrrr/langchain_utils/blob/2dd4cdf88dd6c11d5f882db6490e302bf2bdf961/examples/neo4j_vector_store.ipynb). Then we will use it as a retriever and show how we can create a simple chain usign this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "database = \"graphrag\"  # default index name\n",
    "## Uncomment de wanted embedding model\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # You can specify any sentence-transformer model from the hub\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "\n",
    "# The vector index name was assigned by default\n",
    "store = Neo4jVector.from_existing_index(\n",
    "    embeddings,\n",
    "    url=os.environ[\"NEO4J_URL\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "    index_name=\"vector\",\n",
    "    search_type=\"hybrid\",\n",
    "    keyword_index_name=\"keyword\",\n",
    "    database=database\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = store.as_retriever(search_kwargs= {'k':2, 'score_threshold':0.7}) #, score_threshold=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_path': 'visualization\\\\plotly\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\n\\nParameters:\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\n- class_names (list of str): List of class names corresponding to the labels.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'),\n",
       " Document(metadata={'file_path': 'visualization\\\\plotly\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \"\"\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \"\"\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\"all\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\"all\")      fig.update_layout(title=f\\'Violin Plot of {value_column} by {group_column}\\' if group_column else f\\'Violin Plot of {value_column}\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\n\\nParameters:\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\n- value_column (str): The column name of the DataFrame to be plotted.\\n- group_column (str, optional): The column name for grouping data into different violins.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"How to generate a confusion matrix with plotly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we just define a simple prompt because after it we will refine it\n",
    "\n",
    "prompt_handle_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\n",
    "Context: {context}\n",
    "\n",
    "User question: {question}\n",
    "\"\"\")\n",
    "semantic_chain = {'context':retriever,'question':RunnablePassthrough()} | prompt_handle_conver | llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To generate a confusion matrix with Plotly, you can use the provided function `plot_confusion_matrix` from the `machine_learning_evaluation_plots.py` file. This function takes in a numpy array containing the confusion matrix information and a list of class names corresponding to the labels. It then generates a Plotly figure object for the confusion matrix plot. You can call this function with your confusion matrix array and class names to visualize the confusion matrix using Plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 675, 'total_tokens': 766}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e47669dd-c87a-414a-82b8-ae4bdb09e210-0', usage_metadata={'input_tokens': 675, 'output_tokens': 91, 'total_tokens': 766})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chain.invoke(\"How to generate a confusion matrix with plotly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Adding Memory<br><div>\n",
    "\n",
    "As the final objective of our chain is to handle a conversation (Chatbot), we will add a memory component to the chain. We will show how to integrate this memory (a way of doing it), in both our two cases. This part will also cover the query rewritting functionality, based in the method that we want to apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic search Chain \n",
    "\n",
    "To add memory to this step we will just add another call to an LLM with all the conversation provided by the RunnableWithMessageHistory wrapper. This call will have the objective to summarise the latests 3 (customizable) messages, to create a final message with full context and also already oriented for the purpose of querying the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "prompt_handle_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\n",
    "Context: {context}\n",
    "\n",
    "User question: {question}\n",
    "\"\"\")\n",
    "\n",
    "prompt_summarise_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are going to be provided with several consecutive messages of a user from a conversation.\n",
    "Your task is to contextualize the latest message with anything relevant from the latest if it is necessary.\n",
    "Also try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\n",
    "                                                        \n",
    "History of messages: {chat_history}\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we handle the history summarisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_last_n_messages(chat_history,n=3):\n",
    "    print(chat_history)\n",
    "    if len(chat_history) <3:\n",
    "        return chat_history\n",
    "    else:\n",
    "        print(\"Shortering chat messages\")\n",
    "        print(chat_history)\n",
    "        print(chat_history[-3:])\n",
    "        return chat_history[-3:]\n",
    "    \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    {'chat_history': itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input\": itemgetter('input') | RunnablePassthrough()  } | prompt_summarise_conver | llm,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='No history of messages provided.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a5523a95-4878-46c1-a032-5d47a28949cd-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({ \"input\": \"What is plotly?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What is plotly?'), AIMessage(content='No history of messages provided.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a5523a95-4878-46c1-a032-5d47a28949cd-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='User is asking about what plotly is.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 225, 'total_tokens': 234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ca16c2ed-370e-48d8-b45f-2eb5c82b9539-0', usage_metadata={'input_tokens': 225, 'output_tokens': 9, 'total_tokens': 234})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({ \"input\": \"And how to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What is plotly?'), AIMessage(content='No history of messages provided.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a5523a95-4878-46c1-a032-5d47a28949cd-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85}), HumanMessage(content='And how to create a confusion matrix with it?'), AIMessage(content='User is asking about what plotly is.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 225, 'total_tokens': 234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ca16c2ed-370e-48d8-b45f-2eb5c82b9539-0', usage_metadata={'input_tokens': 225, 'output_tokens': 9, 'total_tokens': 234})]\n",
      "Shortering chat messages\n",
      "[HumanMessage(content='What is plotly?'), AIMessage(content='No history of messages provided.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a5523a95-4878-46c1-a032-5d47a28949cd-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85}), HumanMessage(content='And how to create a confusion matrix with it?'), AIMessage(content='User is asking about what plotly is.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 225, 'total_tokens': 234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ca16c2ed-370e-48d8-b45f-2eb5c82b9539-0', usage_metadata={'input_tokens': 225, 'output_tokens': 9, 'total_tokens': 234})]\n",
      "[AIMessage(content='No history of messages provided.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a5523a95-4878-46c1-a032-5d47a28949cd-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85}), HumanMessage(content='And how to create a confusion matrix with it?'), AIMessage(content='User is asking about what plotly is.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 225, 'total_tokens': 234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ca16c2ed-370e-48d8-b45f-2eb5c82b9539-0', usage_metadata={'input_tokens': 225, 'output_tokens': 9, 'total_tokens': 234})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='User is asking about how to create a confusion matrix using Plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 370, 'total_tokens': 384}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e7e94c1d-5575-46db-b5e6-1fa911ed0c47-0', usage_metadata={'input_tokens': 370, 'output_tokens': 14, 'total_tokens': 384})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({ \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have just seen, this does not make sense, because the variable under 'chat_history' provided by the wrapper is not considering the current message. So if we want to summarise the information so it helps contextualize the current message what we will need is to use both historic and new messages.\n",
    "\n",
    "Also we are going to keep just the previous human messages, because our only objective as we said, is improve both the query to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reset the memory store:\n",
    "\n",
    "store = {}\n",
    "## Modify the function so we just get human messages\n",
    "def select_last_n_messages(chat_history,n=3):\n",
    "\n",
    "    # Extract content from the last 3 HumanMessages\n",
    "    human_contents = [msg.content for msg in chat_history if isinstance(msg, HumanMessage)]\n",
    "    # print(chat_history)\n",
    "    if len(human_contents) <3:\n",
    "        return \"/n\".join(human_contents)\n",
    "    else:\n",
    "        print(\"Shortering chat messages\")\n",
    "        print(human_contents)\n",
    "        print(human_contents[-3:])\n",
    "        return \"/n\".join(human_contents[-3:])\n",
    "    \n",
    "# Modify the prompt to summarise both history and current message together:\n",
    "prompt_summarise_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\n",
    "Your task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \n",
    "You should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\n",
    "\n",
    "Also try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\n",
    "                                                        \n",
    "History of messages: {chat_history}\n",
    "                                                        \n",
    "Current message: {input_message}\n",
    "                                                        \n",
    "Final Message: \n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "   {\"chat_history\": itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input_message\": itemgetter('input'), \"input\": itemgetter('input') | RunnablePassthrough()  } | prompt_summarise_conver | llm,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = with_message_history.invoke({ \"input\": \"What is plotly?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is plotly? (Context: User is asking for information about the software or tool called Plotly.)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the result is also containing text so the query to the vector database is optimized. This obviously could be improved. Also it would be better to use a query for the database and other for the LLM as user input. But right now, and considering that the queries would be pretty simple, we will keep the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = with_message_history.invoke({ \"input\": \"How to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to create a confusion matrix with Plotly?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add our retriever after this newly generated query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "   {\"chat_history\": itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input_message\": itemgetter('input'), \"input\": itemgetter('input') | RunnablePassthrough()  } | prompt_summarise_conver | llm | StrOutputParser() | retriever,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_path': 'visualization\\\\plotly\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\n\\nParameters:\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\n- class_names (list of str): List of class names corresponding to the labels.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'),\n",
       " Document(metadata={'file_path': 'visualization\\\\plotly\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \"\"\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \"\"\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\"all\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\"all\")      fig.update_layout(title=f\\'Violin Plot of {value_column} by {group_column}\\' if group_column else f\\'Violin Plot of {value_column}\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\n\\nParameters:\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\n- value_column (str): The column name of the DataFrame to be plotted.\\n- group_column (str, optional): The column name for grouping data into different violins.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({ \"input\": \"How to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a response from this information\n",
    "\n",
    "Finally, after condensing the latest messages, and rewritting (if necessary) the query, we have our information retrieved from the Vector Database. Now we just need to add the Conversational LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortering chat messages\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?']\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?']\n",
      "Shortering chat messages\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?', 'What is plotly?']\n",
      "['How to create a confusion matrix with it?', 'How to create a confusion matrix with it?', 'What is plotly?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_handle_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\n",
    "Questions are most likely related to coding doubts. You should provide code examples whenever possible.\n",
    "Context: {context}\n",
    "\n",
    "User question: {input}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "   {\"context\": {\"chat_history\": itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input_message\": itemgetter('input')}| prompt_summarise_conver | llm | StrOutputParser() | retriever , \"input\": RunnablePassthrough() }  | prompt_handle_conver | llm ,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "\n",
    "output = with_message_history.invoke({ \"input\": \"What is plotly?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    "\n",
    "output = with_message_history.invoke({ \"input\": \"How to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To create a confusion matrix plot using Plotly, you can use the provided function `plot_confusion_matrix` defined in the code snippet. Here\\'s how you can do it:\\n\\n```python\\ndef plot_confusion_matrix(confusion_matrix, class_names):\\n    \"\"\"\\n    Generates a confusion matrix plot from a sklearn confusion matrix object.\\n    \\n    Parameters:\\n    - confusion_matrix (array): Numpy array containing the confusion matrix information.\\n    - class_names (list of str): List of class names corresponding to the labels.\\n    \\n    Returns:\\n    - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.\\n    \"\"\"\\n    fig = ff.create_annotated_heatmap(\\n        z=confusion_matrix,\\n        x=class_names,\\n        y=class_names,\\n        colorscale=\\'Blues\\',\\n        showscale=True\\n    )\\n    fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')\\n    fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')\\n    return fig\\n\\n# Call the function with your confusion matrix array and class names list\\nconfusion_matrix = [[10, 2, 3], [1, 15, 6], [4, 7, 20]]  # Example confusion matrix data\\nclass_names = [\\'Class A\\', \\'Class B\\', \\'Class C\\']  # Example class names\\nconfusion_matrix_plot = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Display the confusion matrix plot\\nconfusion_matrix_plot.show()\\n```\\n\\nYou can customize the input `confusion_matrix` and `class_names` according to your specific data and labels to generate the confusion matrix plot using Plotly.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Final Chain<br><div>\n",
    "\n",
    "Now that we have showed how to combine the memory with the retriever to generate a full chain, we will add in our retrieving chain also the Graph retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [12ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Shortering chat messages\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?']\n",
      "['How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?']\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] [12ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\",\n",
      "  \"input_message\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\",\n",
      "  \"input_message\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\\n                                                        \\nCurrent message: What is plotly?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] [971ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 16,\n",
      "                \"prompt_tokens\": 165,\n",
      "                \"total_tokens\": 181\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c6fc7f93-ed5e-48c4-9f12-e86a0fd02f9c-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 165,\n",
      "              \"output_tokens\": 16,\n",
      "              \"total_tokens\": 181\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 16,\n",
      "      \"prompt_tokens\": 165,\n",
      "      \"total_tokens\": 181\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:get_schema] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:get_schema] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    You are a Cypher language expert.\\n    Your Task:Generate Cypher statement to query a graph database.\\n    To better contextualize, the Graph database is mapping the Data Science implementations using python\\n    and is divided in the following entities:\\n\\n    Instructions:\\n    Use only the provided relationship types and properties in the schema.\\n    Do not use any other relationship types or properties that are not provided.\\n    Schema:\\n    Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\\n    Note: Do not include any explanations or apologies in your responses.\\n    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\n    Do not include any text except the generated Cypher statement.\\n        - Data Preprocessing Area: Nodes labeled as 'Area' representing areas of Data Science, like 'Data Visualization' or 'Data Preprocessing'.\\n        - SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing. This field is optional, some of the nodes may not have a relation with a 'SubArea' node, so generally \\n        should not be added in the query.\\n        - Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\\n        - Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\\n        - Function: Nodes labeled as 'Function' representing custom functions built on top of those frameworks.\\n    Nodes do not neccesarily have parents of each type of label.\\n\\n    Your main focus should be to identify the Framework and the Function that is being asked.\\n    The question is:\\n    What is plotly and how can it be used to create a confusion matrix?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > llm:ChatOpenAI] [899ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"MATCH (f:Framework{name: \\\"plotly\\\"})-[:CONTAINS_FUNCTION]->(func:Function{name: \\\"create_confusion_matrix\\\"})\\nRETURN f, func\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"MATCH (f:Framework{name: \\\"plotly\\\"})-[:CONTAINS_FUNCTION]->(func:Function{name: \\\"create_confusion_matrix\\\"})\\nRETURN f, func\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 33,\n",
      "                \"prompt_tokens\": 473,\n",
      "                \"total_tokens\": 506\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-1c97464f-9755-471e-b99e-83a946698423-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 473,\n",
      "              \"output_tokens\": 33,\n",
      "              \"total_tokens\": 506\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 33,\n",
      "      \"prompt_tokens\": 473,\n",
      "      \"total_tokens\": 506\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:run_cypher_query] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Generated query----> MATCH (f:Framework{name: \"plotly\"})-[:CONTAINS_FUNCTION]->(func:Function{name: \"create_confusion_matrix\"})\n",
      "RETURN f, func\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:run_cypher_query] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence] [914ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context>] [925ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:context_unifier] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Graph context---> []\n",
      "Vector context---> [Document(metadata={'file_path': 'visualization\\\\plotly\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\n\\nParameters:\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\n- class_names (list of str): List of class names corresponding to the labels.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\plotly\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \"\"\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \"\"\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\"all\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\"all\")      fig.update_layout(title=f\\'Violin Plot of {value_column} by {group_column}\\' if group_column else f\\'Violin Plot of {value_column}\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\n\\nParameters:\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\n- value_column (str): The column name of the DataFrame to be plotted.\\n- group_column (str, optional): The column name for grouping data into different violins.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:context_unifier] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] [1.93s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] [1.96s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\\nQuestions are most likely related to coding doubts. You should provide code examples whenever possible.\\nContext: [Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\\n\\nUser question: What is plotly?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [1.45s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Plotly is a Python library that allows you to create interactive plots and visualizations. It provides a variety of chart types and customization options for creating visually appealing and informative plots. In the context provided, Plotly is used to generate interactive plots for machine learning evaluation metrics like confusion matrices and statistical analysis plots like violin plots. Would you like to see an example of how Plotly is used in code to create a plot?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Plotly is a Python library that allows you to create interactive plots and visualizations. It provides a variety of chart types and customization options for creating visually appealing and informative plots. In the context provided, Plotly is used to generate interactive plots for machine learning evaluation metrics like confusion matrices and statistical analysis plots like violin plots. Would you like to see an example of how Plotly is used in code to create a plot?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 84,\n",
      "                \"prompt_tokens\": 687,\n",
      "                \"total_tokens\": 771\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-02a83970-5763-4019-9fff-4fc1a0cdac01-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 687,\n",
      "              \"output_tokens\": 84,\n",
      "              \"total_tokens\": 771\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 84,\n",
      "      \"prompt_tokens\": 687,\n",
      "      \"total_tokens\": 771\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [3.43s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [3.45s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [3.48s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [7ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Shortering chat messages\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?', 'What is plotly?']\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'What is plotly?']\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] [9ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\\n                                                        \\nCurrent message: How to create a confusion matrix with it?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] [813ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"How to create a confusion matrix with Plotly?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"How to create a confusion matrix with Plotly?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 10,\n",
      "                \"prompt_tokens\": 165,\n",
      "                \"total_tokens\": 175\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5f27565c-c760-46ad-b6b8-7f4b0781baa3-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 165,\n",
      "              \"output_tokens\": 10,\n",
      "              \"total_tokens\": 175\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 10,\n",
      "      \"prompt_tokens\": 165,\n",
      "      \"total_tokens\": 175\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:get_schema] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:get_schema] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema>] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to create a confusion matrix with Plotly?\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to create a confusion matrix with Plotly?\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    You are a Cypher language expert.\\n    Your Task:Generate Cypher statement to query a graph database.\\n    To better contextualize, the Graph database is mapping the Data Science implementations using python\\n    and is divided in the following entities:\\n\\n    Instructions:\\n    Use only the provided relationship types and properties in the schema.\\n    Do not use any other relationship types or properties that are not provided.\\n    Schema:\\n    Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\\n    Note: Do not include any explanations or apologies in your responses.\\n    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\n    Do not include any text except the generated Cypher statement.\\n        - Data Preprocessing Area: Nodes labeled as 'Area' representing areas of Data Science, like 'Data Visualization' or 'Data Preprocessing'.\\n        - SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing. This field is optional, some of the nodes may not have a relation with a 'SubArea' node, so generally \\n        should not be added in the query.\\n        - Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\\n        - Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\\n        - Function: Nodes labeled as 'Function' representing custom functions built on top of those frameworks.\\n    Nodes do not neccesarily have parents of each type of label.\\n\\n    Your main focus should be to identify the Framework and the Function that is being asked.\\n    The question is:\\n    How to create a confusion matrix with Plotly?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > llm:ChatOpenAI] [1.05s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\\nWHERE f.name = \\\"Plotly\\\" AND func.description CONTAINS \\\"confusion matrix\\\"\\nRETURN func, f\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\\nWHERE f.name = \\\"Plotly\\\" AND func.description CONTAINS \\\"confusion matrix\\\"\\nRETURN func, f\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 49,\n",
      "                \"prompt_tokens\": 467,\n",
      "                \"total_tokens\": 516\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8ef52e64-c206-4c8e-9253-abaebbf525d3-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 467,\n",
      "              \"output_tokens\": 49,\n",
      "              \"total_tokens\": 516\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 49,\n",
      "      \"prompt_tokens\": 467,\n",
      "      \"total_tokens\": 516\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:run_cypher_query] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Generated query----> MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\n",
      "WHERE f.name = \"Plotly\" AND func.description CONTAINS \"confusion matrix\"\n",
      "RETURN func, f\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:run_cypher_query] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence] [1.06s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context>] [1.08s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:context_unifier] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Graph context---> []\n",
      "Vector context---> [Document(metadata={'file_path': 'visualization\\\\plotly\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\n\\nParameters:\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\n- class_names (list of str): List of class names corresponding to the labels.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\plotly\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \"\"\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \"\"\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\"all\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\"all\")      fig.update_layout(title=f\\'Violin Plot of {value_column} by {group_column}\\' if group_column else f\\'Violin Plot of {value_column}\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\n\\nParameters:\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\n- value_column (str): The column name of the DataFrame to be plotted.\\n- group_column (str, optional): The column name for grouping data into different violins.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:context_unifier] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] [1.92s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] [1.94s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\\nQuestions are most likely related to coding doubts. You should provide code examples whenever possible.\\nContext: [Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\\n\\nUser question: How to create a confusion matrix with it?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [3.11s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To create a confusion matrix using the provided function `plot_confusion_matrix` from the file `machine_learning_evaluation_plots.py`, you can follow these steps:\\n\\n1. Make sure you have a sklearn confusion matrix object and a list of class names ready.\\n2. Call the `plot_confusion_matrix` function with the confusion matrix and class names as parameters.\\n3. The function will return a Plotly figure object representing the confusion matrix plot.\\n\\nHere is an example code snippet demonstrating how to create a confusion matrix using the `plot_confusion_matrix` function:\\n\\n```python\\nimport plotly.graph_objects as go\\nimport numpy as np\\n\\n# Sample confusion matrix and class names\\nconfusion_matrix = np.array([[10, 2, 3],\\n                              [1, 15, 0],\\n                              [2, 1, 12]])\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Display the confusion matrix plot\\nfig.show()\\n```\\n\\nBy running this code with your actual confusion matrix and class names, you will be able to visualize the confusion matrix using Plotly.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To create a confusion matrix using the provided function `plot_confusion_matrix` from the file `machine_learning_evaluation_plots.py`, you can follow these steps:\\n\\n1. Make sure you have a sklearn confusion matrix object and a list of class names ready.\\n2. Call the `plot_confusion_matrix` function with the confusion matrix and class names as parameters.\\n3. The function will return a Plotly figure object representing the confusion matrix plot.\\n\\nHere is an example code snippet demonstrating how to create a confusion matrix using the `plot_confusion_matrix` function:\\n\\n```python\\nimport plotly.graph_objects as go\\nimport numpy as np\\n\\n# Sample confusion matrix and class names\\nconfusion_matrix = np.array([[10, 2, 3],\\n                              [1, 15, 0],\\n                              [2, 1, 12]])\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Display the confusion matrix plot\\nfig.show()\\n```\\n\\nBy running this code with your actual confusion matrix and class names, you will be able to visualize the confusion matrix using Plotly.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 245,\n",
      "                \"prompt_tokens\": 691,\n",
      "                \"total_tokens\": 936\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5e6a9cb7-b9f0-4f17-aab1-43a8b6d0a972-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 691,\n",
      "              \"output_tokens\": 245,\n",
      "              \"total_tokens\": 936\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 245,\n",
      "      \"prompt_tokens\": 691,\n",
      "      \"total_tokens\": 936\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [5.07s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [5.10s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [5.11s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "langchain.debug = True\n",
    "graph_retriever_chain =  cypher_gen_prompt | llm | RunnableLambda(run_cypher_query)\n",
    "\n",
    "# In this case we also add the rephrasing/summarising from the history:\n",
    "summarisation_chain =  {\"chat_history\": itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input_message\": itemgetter('input')}| prompt_summarise_conver | llm | StrOutputParser()\n",
    "\n",
    "# Therefore the final chain will by:\n",
    "\n",
    "def context_unifier(full_context):\n",
    "    print(\"Graph context--->\",full_context['graph_context'])\n",
    "    print(\"Vector context--->\",full_context['vector_context'])\n",
    "    unified_context = full_context['graph_context'] + full_context['vector_context'] \n",
    "    return unified_context\n",
    "\n",
    "def get_schema(summarisation):\n",
    "    return graph.get_schema\n",
    "\n",
    "final_chain =  {'context' :summarisation_chain | {'graph_context': {'question': RunnablePassthrough(), 'schema': RunnableLambda(get_schema)} | graph_retriever_chain , 'vector_context':retriever} | RunnableLambda(context_unifier), 'input': itemgetter(\"input\")} | prompt_handle_conver | llm\n",
    "final_chain_history = RunnableWithMessageHistory(\n",
    "   final_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "\n",
    "output = final_chain_history.invoke({ \"input\": \"What is plotly?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    "\n",
    "output = final_chain_history.invoke({ \"input\": \"How to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To create a confusion matrix plot using Plotly, you can use the provided function `plot_confusion_matrix` defined in the code snippet. Here\\'s how you can do it:\\n\\n```python\\ndef plot_confusion_matrix(confusion_matrix, class_names):\\n    \"\"\"\\n    Generates a confusion matrix plot from a sklearn confusion matrix object.\\n    \\n    Parameters:\\n    - confusion_matrix (array): Numpy array containing the confusion matrix information.\\n    - class_names (list of str): List of class names corresponding to the labels.\\n    \\n    Returns:\\n    - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.\\n    \"\"\"\\n    fig = ff.create_annotated_heatmap(\\n        z=confusion_matrix,\\n        x=class_names,\\n        y=class_names,\\n        colorscale=\\'Blues\\',\\n        showscale=True\\n    )\\n    fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')\\n    fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')\\n    return fig\\n\\n# Call the function with your confusion matrix array and class names list\\nconfusion_matrix = [[10, 2, 3], [1, 15, 6], [4, 7, 20]]  # Example confusion matrix data\\nclass_names = [\\'Class A\\', \\'Class B\\', \\'Class C\\']  # Example class names\\nconfusion_matrix_plot = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Display the confusion matrix plot\\nconfusion_matrix_plot.show()\\n```\\n\\nYou can customize the input `confusion_matrix` and `class_names` according to your specific data and labels to generate the confusion matrix plot using Plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 360, 'prompt_tokens': 2498, 'total_tokens': 2858}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cf52a879-21f9-4547-89f0-18b8e676540e-0', usage_metadata={'input_tokens': 2498, 'output_tokens': 360, 'total_tokens': 2858})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Next Steps<br><div>\n",
    "\n",
    "This is the initial development for the project ['Chat with your code'](https://github.com/hectorrrr/chat_with_your_code), but there are many potential improvements which we will address over a period of time. Here we list some of them:\n",
    "\n",
    "* Custom query 'Rewritting' for both Graph and Vector search\n",
    "* Test a 'Sequential' retriever instead of parallel, to not get duplicated data if unnecessary.\n",
    "* Create evaluation metrics to optimize the flow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
