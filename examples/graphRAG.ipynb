{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:white; font-size:180%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > TABLE OF CONTENTS<br><div>\n",
    "* [IMPORTS](#1)\n",
    "* [Introduction](#2)\n",
    "* [GraphQA Chain](#3)\n",
    "* [Custom Chain](#4)\n",
    "* [Semantic Retrieval](#5)\n",
    "* [Adding Memory](#6)\n",
    "* [Final Chain](#7)\n",
    "* [Next Steps](#8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import langchain\n",
    "## Chains\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "from langchain_community.chains.graph_qa.prompts import (\n",
    "    CYPHER_QA_PROMPT,\n",
    "    CYPHER_GENERATION_PROMPT\n",
    ")\n",
    "\n",
    "# Prompts:\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "\n",
    "## LLMs:\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Memory\n",
    "## Memory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Introduction<br><div>\n",
    "\n",
    "In this notebook we are going to show how to create a 'Custom' GraphRAG set up, using as an example the GraphQAChain client provided by Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > GraphQA Chain<br><div>\n",
    "\n",
    "We have based this development in the [GraphCypherQAChain](https://api.python.langchain.com/en/latest/chains/langchain_community.chains.graph_qa.cypher.GraphCypherQAChain.html) chain. We are going to show how to replicate it's main behaviour here, but adapting it to the LCEL langchain notation, considering that those old chains are deprecated. In this way we ensure that our final solution will be more 'production ready', and also will be more customized.\n",
    "\n",
    "\n",
    "Inspecting the chain class definition we realized that, by default, it uses two predefined prompts:\n",
    "\n",
    "* Query generation prompt: Handles the conversion from a user query to a CYPHER query\n",
    "\n",
    "* QuestionAnswer prompt: Once the context has been retrieved from our Knowledge graph, this prompt handles the conversation\n",
    "\n",
    "Let's take a look to this prompts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question', 'schema'], template='Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CYPHER_GENERATION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\nThis will be the schema of the graph\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\nHow to build a confusion matrix with plotly?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CYPHER_GENERATION_PROMPT.invoke({'question':\"How to build a confusion matrix with plotly?\",'schema':'This will be the schema of the graph'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nHere is an example:\\n\\nQuestion: Which managers own Neo4j stocks?\\nContext:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\\nHelpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\\n\\nFollow this example when generating answers.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CYPHER_QA_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this second prompt is just to handle the conversation, once the query has retrieved some content, so we will focus in the first one.\n",
    "\n",
    "<a id=\"4\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Custom Chain<br><div>\n",
    "\n",
    "We are going to replicate here a chain that has mainly the same behaviour, but adapted to our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\") # gpt-4-0125-preview occasionally has issues\n",
    "\n",
    "graph_cypher_chain = CYPHER_GENERATION_PROMPT | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph_cypher_chain.invoke({'question':\"How to build a confusion matrix with plotly?\",'schema':'This will be the schema of the graph'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATCH (a:Actual)-[r:PREDICTED_AS]->(p:Predicted)\\nWITH {label: a.label, prediction: p.label} as data, count(*) as count\\nRETURN data, count'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this value makes not sense at all, because we have not provided the schema to the LLM yet, let's reproduce this part based in the reference Chain\n",
    "\n",
    "### Load a graph from neo4j\n",
    "\n",
    "We do this with the wrapper that langchain community offers. We could create our own, but for now we will just stick to it.\n",
    "The main advantage of this client is that directly create an schema for us, so we can contextualize the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.environ['NEO4J_PASSWORD'],database='graphrag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function allows us to directly get the schema from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_props': {'Function': [{'property': 'description', 'type': 'STRING'},\n",
       "   {'property': 'embedding', 'type': 'LIST'},\n",
       "   {'property': 'name', 'type': 'STRING'},\n",
       "   {'property': 'code', 'type': 'STRING'},\n",
       "   {'property': 'file_path', 'type': 'STRING'}],\n",
       "  'Area': [{'property': 'name', 'type': 'STRING'}],\n",
       "  'SubArea': [{'property': 'name', 'type': 'STRING'}],\n",
       "  'Framework': [{'property': 'name', 'type': 'STRING'}],\n",
       "  'Class': [{'property': 'description', 'type': 'STRING'},\n",
       "   {'property': 'name', 'type': 'STRING'},\n",
       "   {'property': 'code', 'type': 'STRING'},\n",
       "   {'property': 'file_path', 'type': 'STRING'}]},\n",
       " 'rel_props': {},\n",
       " 'relationships': [{'start': 'Area',\n",
       "   'type': 'CONTAINS_SUBAREA',\n",
       "   'end': 'SubArea'},\n",
       "  {'start': 'Area', 'type': 'CONTAINS_FRAMEWORK', 'end': 'Framework'},\n",
       "  {'start': 'SubArea', 'type': 'CONTAINS_FRAMEWORK', 'end': 'Framework'},\n",
       "  {'start': 'Framework', 'type': 'CONTAINS_FUNCTION', 'end': 'Function'},\n",
       "  {'start': 'Framework', 'type': 'CONTAINS_CLASS', 'end': 'Class'}],\n",
       " 'metadata': {'constraint': [], 'index': []}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_structured_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the function with this schema as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph_cypher_chain.invoke({'question':\"How to use plotly framework?\",'schema':graph.get_schema})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework {name: \"plotly\"})\\nRETURN f'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other advantage of the langchain graph client is that allows to directly run the queries returned by this first LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = graph.query(result.content)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'f': {'name': 'plotly'}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we added the keyword 'framework' in the question, but that is hightly unlikely in a normal query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MATCH (a:Area)-[:CONTAINS_SUBAREA]->(sa:SubArea)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\\nWHERE func.name = 'plotly'\\nRETURN a, sa, f, func;\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph_cypher_chain.invoke({'question':\"How to use plotly?\",'schema':graph.get_schema})\n",
    "\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = graph.query(result.content)[:5]\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first problem that we see is that considering how our graph is built, the entities are difficult to assign only by their name. This is a generic problem of this kind of solution. Entities should have a very descriptive name (Person, Organization...) so they can be eassily identified by a general LLM. So we should try and contextualize better about the entities that the LLM can expect. For that we will take as reference the base prompt used in the GraphCypherChain and add the following lines defining the entities for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a helpful assistant that understands the context of data science and can generate Cypher queries to retrieve information from a Neo4j database.\n",
    "\n",
    "The database schema includes the following entities:\n",
    "- Data Preprocessing Area: Nodes labeled as 'DataPreprocessingArea' representing areas of data preprocessing.\n",
    "- SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing.\n",
    "- Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\n",
    "- Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\n",
    "- Function: Nodes labeled as 'Function' representing specific functions within frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cypher_gen_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a Cypher language expert.\n",
    "    Your Task:Generate Cypher statement to query a graph database.\n",
    "    To better contextualize, the Graph database is mapping the Data Science implementations using python\n",
    "    and is divided in the following entities:\n",
    "\n",
    "    Instructions:\n",
    "    Use only the provided relationship types and properties in the schema.\n",
    "    Do not use any other relationship types or properties that are not provided.\n",
    "    Schema:\n",
    "    {schema}\n",
    "    Note: Do not include any explanations or apologies in your responses.\n",
    "    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "    Do not include any text except the generated Cypher statement.\n",
    "        - Data Preprocessing Area: Nodes labeled as 'Area' representing areas of Data Science, like 'Data Visualization' or 'Data Preprocessing'.\n",
    "        - SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing. This field is optional, some of the nodes may not have a relation with a 'SubArea' node, so generally \n",
    "        should not be added in the query.\n",
    "        - Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\n",
    "        - Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\n",
    "        - Function: Nodes labeled as 'Function' representing custom functions built on top of those frameworks.\n",
    "    Nodes do not neccesarily have parents of each type of label.\n",
    "\n",
    "    Your main focus should be to identify the Framework and the Function that is being asked.\n",
    "    The question is:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\") # gpt-4-0125-preview occasionally has issues\n",
    "\n",
    "custom_prompt_chain = cypher_gen_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = custom_prompt_chain.invoke({'question':\"How to use plotly?\",'schema':graph.get_schema})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MATCH (:Framework{name: 'plotly'})-[:CONTAINS_FUNCTION]->(f:Function)\\nRETURN f;\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATCH (:Framework{name: \"plotly\"})-[:CONTAINS_FUNCTION]->(:Function{name: \"generate_confusion_matrix\"})\\nRETURN *;'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result = custom_prompt_chain.invoke({'question':\"How to use plotly to generate a confusion matrix?\",'schema':graph.get_schema})\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Generated Cypher Statement is not valid\n{code: Neo.ClientError.Statement.SyntaxError} {message: RETURN * is not allowed when there are no variables in scope (line 2, column 1 (offset: 104))\r\n\"RETURN *;\"\r\n ^}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCypherSyntaxError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:419\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:314\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_disabled_classifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_result\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:221\u001b[0m, in \u001b[0;36mResult._run\u001b[1;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:409\u001b[0m, in \u001b[0;36mResult._attach\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:860\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    857\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    858\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    859\u001b[0m )\n\u001b[1;32m--> 860\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:370\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[1;31mCypherSyntaxError\u001b[0m: {code: Neo.ClientError.Statement.SyntaxError} {message: RETURN * is not allowed when there are no variables in scope (line 2, column 1 (offset: 104))\r\n\"RETURN *;\"\r\n ^}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      2\u001b[0m context\n",
      "File \u001b[1;32mc:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:425\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_data\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CypherSyntaxError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Cypher Statement is not valid\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Generated Cypher Statement is not valid\n{code: Neo.ClientError.Statement.SyntaxError} {message: RETURN * is not allowed when there are no variables in scope (line 2, column 1 (offset: 104))\r\n\"RETURN *;\"\r\n ^}"
     ]
    }
   ],
   "source": [
    "context = graph.query(result.content)[:5]\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An invalid query can sometimes be generated so we would need a validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\\nWHERE f.name = 'plotly' AND func.name = 'plot_confusion_matrix'\\nRETURN func.code\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 477, 'total_tokens': 525}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed4de23b-73ab-4fec-b621-7a53f671b429-0', usage_metadata={'input_tokens': 477, 'output_tokens': 48, 'total_tokens': 525})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result = custom_prompt_chain.invoke({'question':\"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",'schema':graph.get_schema})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'func.code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = graph.query(result.content)[:5]\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that after contextualizing what can be understood as a 'Framework' in our graph, the LLM is correctly identifying plotly as a framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_template =  PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant that helps to form nice and human understandable answers.\n",
    "    The information part contains the provided information that you must use to construct an answer.\n",
    "    The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
    "    Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
    "    If the provided information is empty, say that you don't know the answer.\n",
    "    Information:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sligthly adapted the prompt from the reference chain to our end, removing the example mainly. Let's build the complete chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cypher_query(query):\n",
    "    try:\n",
    "        print(\"Generated query---->\",query.content)\n",
    "        node_contents = graph.query(query.content)[:5]\n",
    "        return node_contents\n",
    "    except: \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    You are a Cypher language expert.\\n    Your Task:Generate Cypher statement to query a graph database.\\n    To better contextualize, the Graph database is mapping the Data Science implementations using python\\n    and is divided in the following entities:\\n\\n    Instructions:\\n    Use only the provided relationship types and properties in the schema.\\n    Do not use any other relationship types or properties that are not provided.\\n    Schema:\\n    Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\\n    Note: Do not include any explanations or apologies in your responses.\\n    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\n    Do not include any text except the generated Cypher statement.\\n        - Data Preprocessing Area: Nodes labeled as 'Area' representing areas of Data Science, like 'Data Visualization' or 'Data Preprocessing'.\\n        - SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing. This field is optional, some of the nodes may not have a relation with a 'SubArea' node, so generally \\n        should not be added in the query.\\n        - Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\\n        - Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\\n        - Function: Nodes labeled as 'Function' representing custom functions built on top of those frameworks.\\n    Nodes do not neccesarily have parents of each type of label.\\n\\n    Your main focus should be to identify the Framework and the Function that is being asked.\\n    The question is:\\n    How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > llm:ChatOpenAI] [889ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\\nWHERE f.name = 'plotly' AND func.name = 'plot_confusion_matrix'\\nRETURN func.code\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\\nWHERE f.name = 'plotly' AND func.name = 'plot_confusion_matrix'\\nRETURN func.code\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 48,\n",
      "                \"prompt_tokens\": 477,\n",
      "                \"total_tokens\": 525\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8a8d6e1d-057e-41e7-8224-3798cbd3ac7b-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 477,\n",
      "              \"output_tokens\": 48,\n",
      "              \"total_tokens\": 525\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 48,\n",
      "      \"prompt_tokens\": 477,\n",
      "      \"total_tokens\": 525\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:run_cypher_query] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Generated query----> MATCH (a:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework)-[:CONTAINS_FUNCTION]->(func:Function)\n",
      "WHERE f.name = 'plotly' AND func.name = 'plot_confusion_matrix'\n",
      "RETURN func.code\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:run_cypher_query] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"func.code\": \"def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale='Blues',         showscale=True     )     fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate='%{text}')     return fig\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] [901ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"func.code\": \"def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale='Blues',         showscale=True     )     fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate='%{text}')     return fig\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [910ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": [\n",
      "    {\n",
      "      \"func.code\": \"def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale='Blues',         showscale=True     )     fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate='%{text}')     return fig\"\n",
      "    }\n",
      "  ],\n",
      "  \"question\": {\n",
      "    \"question\": \"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",\n",
      "    \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": [\n",
      "    {\n",
      "      \"func.code\": \"def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale='Blues',         showscale=True     )     fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate='%{text}')     return fig\"\n",
      "    }\n",
      "  ],\n",
      "  \"question\": {\n",
      "    \"question\": \"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",\n",
      "    \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant that helps to form nice and human understandable answers.\\n    The information part contains the provided information that you must use to construct an answer.\\n    The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\n    Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\\n    If the provided information is empty, say that you don't know the answer.\\n    Information:\\n    [{'func.code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}]\\n    \\n    Question: {'question': \\\"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\\\", 'schema': 'Node properties:\\\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\\\nArea {name: STRING}\\\\nSubArea {name: STRING}\\\\nFramework {name: STRING}\\\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\\\nRelationship properties:\\\\n\\\\nThe relationships:\\\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)'}\\n    Helpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.73s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To generate a 'plot_confusion_matrix' function using plotly, you can use the following code:\\n\\n```python\\ndef plot_confusion_matrix(confusion_matrix, class_names):\\n    fig = ff.create_annotated_heatmap(\\n        z=confusion_matrix,\\n        x=class_names,\\n        y=class_names,\\n        colorscale='Blues',\\n        showscale=True\\n    )\\n    fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')\\n    fig.update_traces(text=confusion_matrix.astype(str), texttemplate='%{text}')\\n    return fig\\n```\\n\\nThis code will generate a confusion matrix plot using the provided sklearn confusion matrix object and class names.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To generate a 'plot_confusion_matrix' function using plotly, you can use the following code:\\n\\n```python\\ndef plot_confusion_matrix(confusion_matrix, class_names):\\n    fig = ff.create_annotated_heatmap(\\n        z=confusion_matrix,\\n        x=class_names,\\n        y=class_names,\\n        colorscale='Blues',\\n        showscale=True\\n    )\\n    fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')\\n    fig.update_traces(text=confusion_matrix.astype(str), texttemplate='%{text}')\\n    return fig\\n```\\n\\nThis code will generate a confusion matrix plot using the provided sklearn confusion matrix object and class names.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 149,\n",
      "                \"prompt_tokens\": 479,\n",
      "                \"total_tokens\": 628\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-43a02990-1d98-4b87-b02d-a4e4f19a6721-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 479,\n",
      "              \"output_tokens\": 149,\n",
      "              \"total_tokens\": 628\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 149,\n",
      "      \"prompt_tokens\": 479,\n",
      "      \"total_tokens\": 628\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To generate a 'plot_confusion_matrix' function using plotly, you can use the following code:\\n\\n```python\\ndef plot_confusion_matrix(confusion_matrix, class_names):\\n    fig = ff.create_annotated_heatmap(\\n        z=confusion_matrix,\\n        x=class_names,\\n        y=class_names,\\n        colorscale='Blues',\\n        showscale=True\\n    )\\n    fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')\\n    fig.update_traces(text=confusion_matrix.astype(str), texttemplate='%{text}')\\n    return fig\\n```\\n\\nThis code will generate a confusion matrix plot using the provided sklearn confusion matrix object and class names.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 479, 'total_tokens': 628}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-43a02990-1d98-4b87-b02d-a4e4f19a6721-0', usage_metadata={'input_tokens': 479, 'output_tokens': 149, 'total_tokens': 628})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_qa_chain = {'context': cypher_gen_prompt | llm | RunnableLambda(run_cypher_query), 'question': RunnablePassthrough()} | qa_prompt_template | llm\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "full_qa_chain.invoke({'question':\"How to use plotly to generate a 'plot_confusion_matrix' function? Provide the code\",'schema':graph.get_schema})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was everything regarding the usage of the Graph based in a query retrieval strategy. But seing that this is not always accurate and may not give any result, we are going to mix it with a 'Semantic simmilarity' retrieval procedure.\n",
    "\n",
    "<a id=\"5\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Semantic retrieval<br><div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to define again an instance of the vector index we created in the [Neo4j Vector Store Tutorial](https://github.com/hectorrrr/langchain_utils/blob/2dd4cdf88dd6c11d5f882db6490e302bf2bdf961/examples/neo4j_vector_store.ipynb). Then we will use it as a retriever and show how we can create a simple chain usign this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\hecto\\OneDrive\\Escritorio\\github_projects\\langchain_utils\\langchain_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "database = \"graphrag\"  # default index name\n",
    "## Uncomment de wanted embedding model\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # You can specify any sentence-transformer model from the hub\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "\n",
    "# The vector index name was assigned by default\n",
    "store = Neo4jVector.from_existing_index(\n",
    "    embeddings,\n",
    "    url=os.environ[\"NEO4J_URL\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "    index_name=\"vector\",\n",
    "    search_type=\"hybrid\",\n",
    "    keyword_index_name=\"keyword\",\n",
    "    database=database\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = store.as_retriever(search_kwargs= {'k':2, 'score_threshold':0.7}) #, score_threshold=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_path': 'visualization\\\\plotly\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\n\\nParameters:\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\n- class_names (list of str): List of class names corresponding to the labels.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'),\n",
       " Document(metadata={'file_path': 'visualization\\\\plotly\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \"\"\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \"\"\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\"all\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\"all\")      fig.update_layout(title=f\\'Violin Plot of {value_column} by {group_column}\\' if group_column else f\\'Violin Plot of {value_column}\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\n\\nParameters:\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\n- value_column (str): The column name of the DataFrame to be plotted.\\n- group_column (str, optional): The column name for grouping data into different violins.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"How to generate a confusion matrix with plotly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we just define a simple prompt because after it we will refine it\n",
    "\n",
    "prompt_handle_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\n",
    "Context: {context}\n",
    "\n",
    "User question: {question}\n",
    "\"\"\")\n",
    "semantic_chain = {'context':retriever,'question':RunnablePassthrough()} | prompt_handle_conver | llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to generate a confusion matrix with plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to generate a confusion matrix with plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to generate a confusion matrix with plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to generate a confusion matrix with plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [56ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\\nContext: [Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\\n\\nUser question: How to generate a confusion matrix with plotly?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.31s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To generate a confusion matrix with Plotly, you can use the provided function `plot_confusion_matrix` from the file `machine_learning_evaluation_plots.py`. This function takes in a numpy array containing the confusion matrix information and a list of class names corresponding to the labels. It then generates a confusion matrix plot using Plotly's annotated heatmap. You can call this function with your confusion matrix array and class names to visualize the confusion matrix.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To generate a confusion matrix with Plotly, you can use the provided function `plot_confusion_matrix` from the file `machine_learning_evaluation_plots.py`. This function takes in a numpy array containing the confusion matrix information and a list of class names corresponding to the labels. It then generates a confusion matrix plot using Plotly's annotated heatmap. You can call this function with your confusion matrix array and class names to visualize the confusion matrix.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 87,\n",
      "                \"prompt_tokens\": 675,\n",
      "                \"total_tokens\": 762\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4cb1dae8-97d9-4878-b5c0-66ef1d200c14-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 675,\n",
      "              \"output_tokens\": 87,\n",
      "              \"total_tokens\": 762\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 87,\n",
      "      \"prompt_tokens\": 675,\n",
      "      \"total_tokens\": 762\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.37s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To generate a confusion matrix with Plotly, you can use the provided function `plot_confusion_matrix` from the file `machine_learning_evaluation_plots.py`. This function takes in a numpy array containing the confusion matrix information and a list of class names corresponding to the labels. It then generates a confusion matrix plot using Plotly's annotated heatmap. You can call this function with your confusion matrix array and class names to visualize the confusion matrix.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 675, 'total_tokens': 762}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4cb1dae8-97d9-4878-b5c0-66ef1d200c14-0', usage_metadata={'input_tokens': 675, 'output_tokens': 87, 'total_tokens': 762})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chain.invoke(\"How to generate a confusion matrix with plotly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Adding Memory<br><div>\n",
    "\n",
    "As the final objective of our chain is to handle a conversation (Chatbot), we will add a memory component to the chain. We will show how to integrate this memory (a way of doing it), in both our two cases. This part will also cover the query rewritting functionality, based in the method that we want to apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic search Chain \n",
    "\n",
    "To add memory to this step we will just add another call to an LLM with all the conversation provided by the RunnableWithMessageHistory wrapper. This call will have the objective to summarise the latests 3 (customizable) messages, to create a final message with full context and also already oriented for the purpose of querying the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "prompt_handle_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\n",
    "Context: {context}\n",
    "\n",
    "User question: {question}\n",
    "\"\"\")\n",
    "\n",
    "prompt_summarise_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are going to be provided with several consecutive messages of a user from a conversation.\n",
    "Your task is to contextualize the latest message with anything relevant from the latest if it is necessary.\n",
    "Also try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\n",
    "                                                        \n",
    "History of messages: {chat_history}\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we handle the history summarisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_last_n_messages(chat_history,n=3):\n",
    "    print(chat_history)\n",
    "    if len(chat_history) <3:\n",
    "        return chat_history\n",
    "    else:\n",
    "        print(\"Shortering chat messages\")\n",
    "        print(chat_history)\n",
    "        print(chat_history[-3:])\n",
    "        return chat_history[-3:]\n",
    "    \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    {'chat_history': itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input\": itemgetter('input') | RunnablePassthrough()  } | prompt_summarise_conver | llm,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [16ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": []\n",
      "}\n",
      "[]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:select_last_n_messages] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input>] [14ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": [],\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": [],\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation.\\nYour task is to contextualize the latest message with anything relevant from the latest if it is necessary.\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: []\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [498ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"No history of messages available.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"No history of messages available.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 6,\n",
      "                \"prompt_tokens\": 79,\n",
      "                \"total_tokens\": 85\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-be84a815-8e68-4b10-958c-34f424455f39-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 79,\n",
      "              \"output_tokens\": 6,\n",
      "              \"total_tokens\": 85\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 6,\n",
      "      \"prompt_tokens\": 79,\n",
      "      \"total_tokens\": 85\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [533ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [559ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [596ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='No history of messages available.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-be84a815-8e68-4b10-958c-34f424455f39-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({ \"input\": \"What is plotly?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"And how to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"And how to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"And how to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"And how to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [17ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"And how to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"And how to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"And how to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"And how to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "[HumanMessage(content='What is plotly?'), AIMessage(content='No history of messages available.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-be84a815-8e68-4b10-958c-34f424455f39-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85})]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:select_last_n_messages] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input>] [12ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation.\\nYour task is to contextualize the latest message with anything relevant from the latest if it is necessary.\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: [HumanMessage(content='What is plotly?'), AIMessage(content='No history of messages available.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-be84a815-8e68-4b10-958c-34f424455f39-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85})]\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [478ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"User is asking about plotly.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"User is asking about plotly.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 7,\n",
      "                \"prompt_tokens\": 225,\n",
      "                \"total_tokens\": 232\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a20f66da-9e5d-4e61-8dab-7d6e3017384a-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 225,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 232\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 7,\n",
      "      \"prompt_tokens\": 225,\n",
      "      \"total_tokens\": 232\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [504ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [530ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [566ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='User is asking about plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 225, 'total_tokens': 232}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a20f66da-9e5d-4e61-8dab-7d6e3017384a-0', usage_metadata={'input_tokens': 225, 'output_tokens': 7, 'total_tokens': 232})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({ \"input\": \"And how to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [18ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnablePassthrough] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "[HumanMessage(content='What is plotly?'), AIMessage(content='No history of messages available.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-be84a815-8e68-4b10-958c-34f424455f39-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85}), HumanMessage(content='And how to create a confusion matrix with it?'), AIMessage(content='User is asking about plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 225, 'total_tokens': 232}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a20f66da-9e5d-4e61-8dab-7d6e3017384a-0', usage_metadata={'input_tokens': 225, 'output_tokens': 7, 'total_tokens': 232})]\n",
      "Shortering chat messages\n",
      "[HumanMessage(content='What is plotly?'), AIMessage(content='No history of messages available.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-be84a815-8e68-4b10-958c-34f424455f39-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85}), HumanMessage(content='And how to create a confusion matrix with it?'), AIMessage(content='User is asking about plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 225, 'total_tokens': 232}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a20f66da-9e5d-4e61-8dab-7d6e3017384a-0', usage_metadata={'input_tokens': 225, 'output_tokens': 7, 'total_tokens': 232})]\n",
      "[AIMessage(content='No history of messages available.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-be84a815-8e68-4b10-958c-34f424455f39-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85}), HumanMessage(content='And how to create a confusion matrix with it?'), AIMessage(content='User is asking about plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 225, 'total_tokens': 232}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a20f66da-9e5d-4e61-8dab-7d6e3017384a-0', usage_metadata={'input_tokens': 225, 'output_tokens': 7, 'total_tokens': 232})]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence > chain:select_last_n_messages] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input> > chain:RunnableSequence] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input>] [21ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation.\\nYour task is to contextualize the latest message with anything relevant from the latest if it is necessary.\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: [AIMessage(content='No history of messages available.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-be84a815-8e68-4b10-958c-34f424455f39-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85}), HumanMessage(content='And how to create a confusion matrix with it?'), AIMessage(content='User is asking about plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 225, 'total_tokens': 232}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a20f66da-9e5d-4e61-8dab-7d6e3017384a-0', usage_metadata={'input_tokens': 225, 'output_tokens': 7, 'total_tokens': 232})]\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [564ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"User is asking about how to create a confusion matrix with plotly.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"User is asking about how to create a confusion matrix with plotly.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 14,\n",
      "                \"prompt_tokens\": 371,\n",
      "                \"total_tokens\": 385\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-940be15f-7fb0-47c7-aa63-6b2fcfc04588-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 371,\n",
      "              \"output_tokens\": 14,\n",
      "              \"total_tokens\": 385\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 14,\n",
      "      \"prompt_tokens\": 371,\n",
      "      \"total_tokens\": 385\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [603ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [625ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [661ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='User is asking about how to create a confusion matrix with plotly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 371, 'total_tokens': 385}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-940be15f-7fb0-47c7-aa63-6b2fcfc04588-0', usage_metadata={'input_tokens': 371, 'output_tokens': 14, 'total_tokens': 385})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({ \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have just seen, this does not make sense, because the variable under 'chat_history' provided by the wrapper is not considering the current message. So if we want to summarise the information so it helps contextualize the current message what we will need is to use both historic and new messages.\n",
    "\n",
    "Also we are going to keep just the previous human messages, because our only objective as we said, is improve both the query to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reset the memory store:\n",
    "\n",
    "store = {}\n",
    "## Modify the function so we just get human messages\n",
    "def select_last_n_messages(chat_history,n=3):\n",
    "\n",
    "    # Extract content from the last 3 HumanMessages\n",
    "    human_contents = [msg.content for msg in chat_history if isinstance(msg, HumanMessage)]\n",
    "    # print(chat_history)\n",
    "    if len(human_contents) <3:\n",
    "        return \"/n\".join(human_contents)\n",
    "    else:\n",
    "        print(\"Shortering chat messages\")\n",
    "        print(human_contents)\n",
    "        print(human_contents[-3:])\n",
    "        return \"/n\".join(human_contents[-3:])\n",
    "    \n",
    "# Modify the prompt to summarise both history and current message together:\n",
    "prompt_summarise_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\n",
    "Your task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \n",
    "You should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\n",
    "\n",
    "Also try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\n",
    "                                                        \n",
    "History of messages: {chat_history}\n",
    "                                                        \n",
    "Current message: {input_message}\n",
    "                                                        \n",
    "Final Message: \n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "   {\"chat_history\": itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input_message\": itemgetter('input'), \"input\": itemgetter('input') | RunnablePassthrough()  } | prompt_summarise_conver | llm,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [21ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableLambda] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] [7ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:select_last_n_messages] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] [9ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input>] [40ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"\",\n",
      "  \"input_message\": \"What is plotly?\",\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"\",\n",
      "  \"input_message\": \"What is plotly?\",\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [3ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: \\n                                                        \\nCurrent message: What is plotly?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [536ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What is plotly?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What is plotly?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 5,\n",
      "                \"prompt_tokens\": 139,\n",
      "                \"total_tokens\": 144\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-85dbae9d-e6bf-4889-b3a4-657562dad9b4-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 139,\n",
      "              \"output_tokens\": 5,\n",
      "              \"total_tokens\": 144\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 5,\n",
      "      \"prompt_tokens\": 139,\n",
      "      \"total_tokens\": 144\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [614ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [651ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [706ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "output = with_message_history.invoke({ \"input\": \"What is plotly?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is plotly?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the result is also containing text so the query to the vector database is optimized. This obviously could be improved. Also it would be better to use a query for the database and other for the LLM as user input. But right now, and considering that the queries would be pretty simple, we will keep the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [12ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:select_last_n_messages] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input>] [21ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\",\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\",\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: What is plotly?\\n                                                        \\nCurrent message: How to create a confusion matrix with it?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [486ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"How to create a confusion matrix with Plotly?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"How to create a confusion matrix with Plotly?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 10,\n",
      "                \"prompt_tokens\": 147,\n",
      "                \"total_tokens\": 157\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8f2403fe-0e5d-4c82-9293-de6ad2d290a0-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 147,\n",
      "              \"output_tokens\": 10,\n",
      "              \"total_tokens\": 157\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 10,\n",
      "      \"prompt_tokens\": 147,\n",
      "      \"total_tokens\": 157\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [521ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [535ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [560ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "output = with_message_history.invoke({ \"input\": \"How to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to create a confusion matrix with Plotly?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add our retriever after this newly generated query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "   {\"chat_history\": itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input_message\": itemgetter('input'), \"input\": itemgetter('input') | RunnablePassthrough()  } | prompt_summarise_conver | llm | StrOutputParser() | retriever,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [22ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence > chain:select_last_n_messages] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?/nHow to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input> > chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?/nHow to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message,input>] [20ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?/nHow to create a confusion matrix with it?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\",\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?/nHow to create a confusion matrix with it?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\",\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: What is plotly?/nHow to create a confusion matrix with it?\\n                                                        \\nCurrent message: How to create a confusion matrix with it?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [488ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"How to create a confusion matrix with plotly?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"How to create a confusion matrix with plotly?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 10,\n",
      "                \"prompt_tokens\": 158,\n",
      "                \"total_tokens\": 168\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f7c2992e-33b8-4a09-8aec-bdbc156d677c-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 158,\n",
      "              \"output_tokens\": 10,\n",
      "              \"total_tokens\": 168\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 10,\n",
      "      \"prompt_tokens\": 158,\n",
      "      \"total_tokens\": 168\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [570ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [603ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [647ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_path': 'visualization\\\\plotly\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\n\\nParameters:\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\n- class_names (list of str): List of class names corresponding to the labels.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'),\n",
       " Document(metadata={'file_path': 'visualization\\\\plotly\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \"\"\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \"\"\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\"all\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\"all\")      fig.update_layout(title=f\\'Violin Plot of {value_column} by {group_column}\\' if group_column else f\\'Violin Plot of {value_column}\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\n\\nParameters:\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\n- value_column (str): The column name of the DataFrame to be plotted.\\n- group_column (str, optional): The column name for grouping data into different violins.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({ \"input\": \"How to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a response from this information\n",
    "\n",
    "Finally, after condensing the latest messages, and rewritting (if necessary) the query, we have our information retrieved from the Vector Database. Now we just need to add the Conversational LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [20ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Shortering chat messages\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?']\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?']\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?/nHow to create a confusion matrix with it?/nHow to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] [7ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?/nHow to create a confusion matrix with it?/nHow to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] [24ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?/nHow to create a confusion matrix with it?/nHow to create a confusion matrix with it?\",\n",
      "  \"input_message\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?/nHow to create a confusion matrix with it?/nHow to create a confusion matrix with it?\",\n",
      "  \"input_message\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: What is plotly?/nHow to create a confusion matrix with it?/nHow to create a confusion matrix with it?\\n                                                        \\nCurrent message: What is plotly?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] [544ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 16,\n",
      "                \"prompt_tokens\": 165,\n",
      "                \"total_tokens\": 181\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ef84961c-1817-4f7f-8571-2b1199b2527b-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 165,\n",
      "              \"output_tokens\": 16,\n",
      "              \"total_tokens\": 181\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 16,\n",
      "      \"prompt_tokens\": 165,\n",
      "      \"total_tokens\": 181\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] [646ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] [686ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\\nQuestions are most likely related to coding doubts. You should provide code examples whenever possible.\\nContext: [Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\\n\\nUser question: {'input': 'What is plotly?', 'chat_history': [HumanMessage(content='What is plotly?'), AIMessage(content='What is plotly?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 139, 'total_tokens': 144}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-85dbae9d-e6bf-4889-b3a4-657562dad9b4-0', usage_metadata={'input_tokens': 139, 'output_tokens': 5, 'total_tokens': 144}), HumanMessage(content='How to create a confusion matrix with it?'), AIMessage(content='How to create a confusion matrix with Plotly?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 147, 'total_tokens': 157}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8f2403fe-0e5d-4c82-9293-de6ad2d290a0-0', usage_metadata={'input_tokens': 147, 'output_tokens': 10, 'total_tokens': 157}), HumanMessage(content='How to create a confusion matrix with it?'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [2.79s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Plotly is a Python library that allows you to create interactive plots and visualizations. It provides a variety of chart types and customization options to create visually appealing and informative plots.\\n\\nTo create a confusion matrix plot using Plotly, you can use the `plot_confusion_matrix` function defined in the `machine_learning_evaluation_plots.py` file. This function takes a confusion matrix as input and generates an annotated heatmap plot representing the confusion matrix.\\n\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\n\\n```python\\n# Import necessary libraries\\nimport plotly.figure_factory as ff\\n\\n# Define your confusion matrix and class names\\nconfusion_matrix = [[10, 2, 3],\\n                    [1, 15, 0],\\n                    [5, 1, 12]]\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Show the plot\\nfig.show()\\n```\\n\\nThis code snippet demonstrates how to create a confusion matrix plot using Plotly. You can customize the appearance of the plot by modifying the parameters in the `plot_confusion_matrix` function.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Plotly is a Python library that allows you to create interactive plots and visualizations. It provides a variety of chart types and customization options to create visually appealing and informative plots.\\n\\nTo create a confusion matrix plot using Plotly, you can use the `plot_confusion_matrix` function defined in the `machine_learning_evaluation_plots.py` file. This function takes a confusion matrix as input and generates an annotated heatmap plot representing the confusion matrix.\\n\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\n\\n```python\\n# Import necessary libraries\\nimport plotly.figure_factory as ff\\n\\n# Define your confusion matrix and class names\\nconfusion_matrix = [[10, 2, 3],\\n                    [1, 15, 0],\\n                    [5, 1, 12]]\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Show the plot\\nfig.show()\\n```\\n\\nThis code snippet demonstrates how to create a confusion matrix plot using Plotly. You can customize the appearance of the plot by modifying the parameters in the `plot_confusion_matrix` function.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 247,\n",
      "                \"prompt_tokens\": 1639,\n",
      "                \"total_tokens\": 1886\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-619d9036-0b59-42df-b041-834f3362350f-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1639,\n",
      "              \"output_tokens\": 247,\n",
      "              \"total_tokens\": 1886\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 247,\n",
      "      \"prompt_tokens\": 1639,\n",
      "      \"total_tokens\": 1886\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [3.52s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [3.57s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [3.61s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [9ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [26ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Shortering chat messages\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?', 'What is plotly?']\n",
      "['How to create a confusion matrix with it?', 'How to create a confusion matrix with it?', 'What is plotly?']\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?/nHow to create a confusion matrix with it?/nWhat is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] [9ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?/nHow to create a confusion matrix with it?/nWhat is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] [33ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"How to create a confusion matrix with it?/nHow to create a confusion matrix with it?/nWhat is plotly?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"How to create a confusion matrix with it?/nHow to create a confusion matrix with it?/nWhat is plotly?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: How to create a confusion matrix with it?/nHow to create a confusion matrix with it?/nWhat is plotly?\\n                                                        \\nCurrent message: How to create a confusion matrix with it?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] [518ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"How to create a confusion matrix with plotly?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"How to create a confusion matrix with plotly?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 10,\n",
      "                \"prompt_tokens\": 169,\n",
      "                \"total_tokens\": 179\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-2d4d4a73-5d98-42d0-966c-4f2a6c10aad4-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 169,\n",
      "              \"output_tokens\": 10,\n",
      "              \"total_tokens\": 179\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 10,\n",
      "      \"prompt_tokens\": 169,\n",
      "      \"total_tokens\": 179\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] [681ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] [726ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\\nQuestions are most likely related to coding doubts. You should provide code examples whenever possible.\\nContext: [Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\\n\\nUser question: {'input': 'How to create a confusion matrix with it?', 'chat_history': [HumanMessage(content='What is plotly?'), AIMessage(content='What is plotly?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 139, 'total_tokens': 144}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-85dbae9d-e6bf-4889-b3a4-657562dad9b4-0', usage_metadata={'input_tokens': 139, 'output_tokens': 5, 'total_tokens': 144}), HumanMessage(content='How to create a confusion matrix with it?'), AIMessage(content='How to create a confusion matrix with Plotly?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 147, 'total_tokens': 157}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8f2403fe-0e5d-4c82-9293-de6ad2d290a0-0', usage_metadata={'input_tokens': 147, 'output_tokens': 10, 'total_tokens': 157}), HumanMessage(content='How to create a confusion matrix with it?'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.'), HumanMessage(content='What is plotly?'), AIMessage(content=\\\"Plotly is a Python library that allows you to create interactive plots and visualizations. It provides a variety of chart types and customization options to create visually appealing and informative plots.\\\\n\\\\nTo create a confusion matrix plot using Plotly, you can use the `plot_confusion_matrix` function defined in the `machine_learning_evaluation_plots.py` file. This function takes a confusion matrix as input and generates an annotated heatmap plot representing the confusion matrix.\\\\n\\\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\\\n\\\\n```python\\\\n# Import necessary libraries\\\\nimport plotly.figure_factory as ff\\\\n\\\\n# Define your confusion matrix and class names\\\\nconfusion_matrix = [[10, 2, 3],\\\\n                    [1, 15, 0],\\\\n                    [5, 1, 12]]\\\\nclass_names = ['Class A', 'Class B', 'Class C']\\\\n\\\\n# Call the plot_confusion_matrix function\\\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\\\n\\\\n# Show the plot\\\\nfig.show()\\\\n```\\\\n\\\\nThis code snippet demonstrates how to create a confusion matrix plot using Plotly. You can customize the appearance of the plot by modifying the parameters in the `plot_confusion_matrix` function.\\\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 247, 'prompt_tokens': 1639, 'total_tokens': 1886}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-619d9036-0b59-42df-b041-834f3362350f-0', usage_metadata={'input_tokens': 1639, 'output_tokens': 247, 'total_tokens': 1886})]}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [2.72s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To create a confusion matrix plot using Plotly, you can use the `plot_confusion_matrix` function defined in the `machine_learning_evaluation_plots.py` file. This function takes a confusion matrix as input and generates an annotated heatmap plot representing the confusion matrix.\\n\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\n\\n```python\\n# Import necessary libraries\\nimport plotly.figure_factory as ff\\n\\n# Define your confusion matrix and class names\\nconfusion_matrix = [[10, 2, 3],\\n                    [1, 15, 0],\\n                    [5, 1, 12]]\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Show the plot\\nfig.show()\\n```\\n\\nThis code snippet demonstrates how to create a confusion matrix plot using Plotly. You can customize the appearance of the plot by modifying the parameters in the `plot_confusion_matrix` function.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To create a confusion matrix plot using Plotly, you can use the `plot_confusion_matrix` function defined in the `machine_learning_evaluation_plots.py` file. This function takes a confusion matrix as input and generates an annotated heatmap plot representing the confusion matrix.\\n\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\n\\n```python\\n# Import necessary libraries\\nimport plotly.figure_factory as ff\\n\\n# Define your confusion matrix and class names\\nconfusion_matrix = [[10, 2, 3],\\n                    [1, 15, 0],\\n                    [5, 1, 12]]\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Show the plot\\nfig.show()\\n```\\n\\nThis code snippet demonstrates how to create a confusion matrix plot using Plotly. You can customize the appearance of the plot by modifying the parameters in the `plot_confusion_matrix` function.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 212,\n",
      "                \"prompt_tokens\": 2054,\n",
      "                \"total_tokens\": 2266\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f4872dd3-fe75-488d-af3e-2bfc8a9c4a0a-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 2054,\n",
      "              \"output_tokens\": 212,\n",
      "              \"total_tokens\": 2266\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 212,\n",
      "      \"prompt_tokens\": 2054,\n",
      "      \"total_tokens\": 2266\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [3.50s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [3.57s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [3.64s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_handle_conver =  PromptTemplate.from_template(\"\"\"\n",
    "You are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\n",
    "Questions are most likely related to coding doubts. You should provide code examples whenever possible.\n",
    "Context: {context}\n",
    "\n",
    "User question: {input}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "   {\"context\": {\"chat_history\": itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input_message\": itemgetter('input')}| prompt_summarise_conver | llm | StrOutputParser() | retriever , \"input\": RunnablePassthrough() }  | prompt_handle_conver | llm ,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "\n",
    "output = with_message_history.invoke({ \"input\": \"What is plotly?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    "\n",
    "output = with_message_history.invoke({ \"input\": \"How to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To create a confusion matrix plot using Plotly, you can use the `plot_confusion_matrix` function defined in the `machine_learning_evaluation_plots.py` file. This function takes a confusion matrix as input and generates an annotated heatmap plot representing the confusion matrix.\\n\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\n\\n```python\\n# Import necessary libraries\\nimport plotly.figure_factory as ff\\n\\n# Define your confusion matrix and class names\\nconfusion_matrix = [[10, 2, 3],\\n                    [1, 15, 0],\\n                    [5, 1, 12]]\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Show the plot\\nfig.show()\\n```\\n\\nThis code snippet demonstrates how to create a confusion matrix plot using Plotly. You can customize the appearance of the plot by modifying the parameters in the `plot_confusion_matrix` function.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Final Chain<br><div>\n",
    "\n",
    "Now that we have showed how to combine the memory with the retriever to generate a full chain, we will add in our retrieving chain also the Graph retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [18ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Shortering chat messages\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?']\n",
      "['How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?']\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] [20ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\",\n",
      "  \"input_message\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\",\n",
      "  \"input_message\": \"What is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: How to create a confusion matrix with it?/nWhat is plotly?/nHow to create a confusion matrix with it?\\n                                                        \\nCurrent message: What is plotly?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] [613ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 16,\n",
      "                \"prompt_tokens\": 165,\n",
      "                \"total_tokens\": 181\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6a8b4f6e-18bf-45fe-b19f-7964db69dc3f-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 165,\n",
      "              \"output_tokens\": 16,\n",
      "              \"total_tokens\": 181\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 16,\n",
      "      \"prompt_tokens\": 165,\n",
      "      \"total_tokens\": 181\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:get_schema] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is plotly and how can it be used to create a confusion matrix?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:get_schema] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema>] [8ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is plotly and how can it be used to create a confusion matrix?\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    You are a Cypher language expert.\\n    Your Task:Generate Cypher statement to query a graph database.\\n    To better contextualize, the Graph database is mapping the Data Science implementations using python\\n    and is divided in the following entities:\\n\\n    Instructions:\\n    Use only the provided relationship types and properties in the schema.\\n    Do not use any other relationship types or properties that are not provided.\\n    Schema:\\n    Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\\n    Note: Do not include any explanations or apologies in your responses.\\n    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\n    Do not include any text except the generated Cypher statement.\\n        - Data Preprocessing Area: Nodes labeled as 'Area' representing areas of Data Science, like 'Data Visualization' or 'Data Preprocessing'.\\n        - SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing. This field is optional, some of the nodes may not have a relation with a 'SubArea' node, so generally \\n        should not be added in the query.\\n        - Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\\n        - Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\\n        - Function: Nodes labeled as 'Function' representing custom functions built on top of those frameworks.\\n    Nodes do not neccesarily have parents of each type of label.\\n\\n    Your main focus should be to identify the Framework and the Function that is being asked.\\n    The question is:\\n    What is plotly and how can it be used to create a confusion matrix?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > llm:ChatOpenAI] [732ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"MATCH (f:Framework {name: \\\"plotly\\\"})-[:CONTAINS_FUNCTION]->(func:Function {description: \\\"create a confusion matrix\\\"})\\nRETURN f, func\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"MATCH (f:Framework {name: \\\"plotly\\\"})-[:CONTAINS_FUNCTION]->(func:Function {description: \\\"create a confusion matrix\\\"})\\nRETURN f, func\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 35,\n",
      "                \"prompt_tokens\": 473,\n",
      "                \"total_tokens\": 508\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-75cedf94-9236-4ccc-b15d-a8bf56dd177e-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 473,\n",
      "              \"output_tokens\": 35,\n",
      "              \"total_tokens\": 508\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 35,\n",
      "      \"prompt_tokens\": 473,\n",
      "      \"total_tokens\": 508\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:run_cypher_query] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Generated query----> MATCH (f:Framework {name: \"plotly\"})-[:CONTAINS_FUNCTION]->(func:Function {description: \"create a confusion matrix\"})\n",
      "RETURN f, func\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:run_cypher_query] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence] [763ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context>] [795ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:context_unifier] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Graph context---> []\n",
      "Vector context---> [Document(metadata={'file_path': 'visualization\\\\plotly\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\n\\nParameters:\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\n- class_names (list of str): List of class names corresponding to the labels.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\plotly\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \"\"\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \"\"\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\"all\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\"all\")      fig.update_layout(title=f\\'Violin Plot of {value_column} by {group_column}\\' if group_column else f\\'Violin Plot of {value_column}\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\n\\nParameters:\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\n- value_column (str): The column name of the DataFrame to be plotted.\\n- group_column (str, optional): The column name for grouping data into different violins.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:context_unifier] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] [1.48s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] [1.52s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\\nQuestions are most likely related to coding doubts. You should provide code examples whenever possible.\\nContext: [Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\\n\\nUser question: What is plotly?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [1.22s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Plotly is a Python library that allows you to create interactive plots and visualizations. It provides a variety of chart types and customization options for creating visually appealing and informative plots. In the context provided, there are examples of functions that use Plotly to generate specific types of plots, such as confusion matrix plots and violin plots. Would you like to see an example of how to use Plotly to create a plot?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Plotly is a Python library that allows you to create interactive plots and visualizations. It provides a variety of chart types and customization options for creating visually appealing and informative plots. In the context provided, there are examples of functions that use Plotly to generate specific types of plots, such as confusion matrix plots and violin plots. Would you like to see an example of how to use Plotly to create a plot?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 83,\n",
      "                \"prompt_tokens\": 687,\n",
      "                \"total_tokens\": 770\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-1e2246de-fd21-4fc9-983a-a6ef518d57fe-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 687,\n",
      "              \"output_tokens\": 83,\n",
      "              \"total_tokens\": 770\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 83,\n",
      "      \"prompt_tokens\": 687,\n",
      "      \"total_tokens\": 770\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [2.80s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [2.89s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [2.93s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:insert_history] [16ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableLambda] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Shortering chat messages\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'How to create a confusion matrix with it?', 'What is plotly?', 'How to create a confusion matrix with it?', 'What is plotly?']\n",
      "['What is plotly?', 'How to create a confusion matrix with it?', 'What is plotly?']\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence > chain:select_last_n_messages] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message> > chain:RunnableSequence] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<chat_history,input_message>] [21ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": \"What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\",\n",
      "  \"input_message\": \"How to create a confusion matrix with it?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are going to be provided with several consecutive messages of a user from a conversation, together with the current one.\\nYour task is to contextualize the current message with anything crutial from the oldest if it is necessary, so it can be understood alone. \\nYou should not change the format of the message and just create a final Input/question from the Current question, not older questions should be added.\\n\\nAlso try to condense the information (without removing anything relevant) so the input is more usable as a query for a vector database.\\n                                                        \\nHistory of messages: What is plotly?/nHow to create a confusion matrix with it?/nWhat is plotly?\\n                                                        \\nCurrent message: How to create a confusion matrix with it?\\n                                                        \\nFinal Message:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > llm:ChatOpenAI] [518ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"How to create a confusion matrix with Plotly?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"How to create a confusion matrix with Plotly?\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 10,\n",
      "                \"prompt_tokens\": 165,\n",
      "                \"total_tokens\": 175\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-38bda192-ebbe-4590-972c-a92f387016e6-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 165,\n",
      "              \"output_tokens\": 10,\n",
      "              \"total_tokens\": 175\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 10,\n",
      "      \"prompt_tokens\": 165,\n",
      "      \"total_tokens\": 175\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:get_schema] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"How to create a confusion matrix with Plotly?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema> > chain:get_schema] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:RunnableParallel<question,schema>] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to create a confusion matrix with Plotly?\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How to create a confusion matrix with Plotly?\",\n",
      "  \"schema\": \"Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    You are a Cypher language expert.\\n    Your Task:Generate Cypher statement to query a graph database.\\n    To better contextualize, the Graph database is mapping the Data Science implementations using python\\n    and is divided in the following entities:\\n\\n    Instructions:\\n    Use only the provided relationship types and properties in the schema.\\n    Do not use any other relationship types or properties that are not provided.\\n    Schema:\\n    Node properties:\\nFunction {description: STRING, embedding: LIST, name: STRING, code: STRING, file_path: STRING}\\nArea {name: STRING}\\nSubArea {name: STRING}\\nFramework {name: STRING}\\nClass {description: STRING, name: STRING, code: STRING, file_path: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Area)-[:CONTAINS_SUBAREA]->(:SubArea)\\n(:Area)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:SubArea)-[:CONTAINS_FRAMEWORK]->(:Framework)\\n(:Framework)-[:CONTAINS_FUNCTION]->(:Function)\\n(:Framework)-[:CONTAINS_CLASS]->(:Class)\\n    Note: Do not include any explanations or apologies in your responses.\\n    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\n    Do not include any text except the generated Cypher statement.\\n        - Data Preprocessing Area: Nodes labeled as 'Area' representing areas of Data Science, like 'Data Visualization' or 'Data Preprocessing'.\\n        - SubArea: Nodes labeled as 'SubArea' representing sub-areas within data preprocessing. This field is optional, some of the nodes may not have a relation with a 'SubArea' node, so generally \\n        should not be added in the query.\\n        - Framework: Nodes labeled as 'Framework' representing frameworks used in data science.\\n        - Class: Nodes labeled as 'Class' representing a set of functions defining a Python class within a framework.\\n        - Function: Nodes labeled as 'Function' representing custom functions built on top of those frameworks.\\n    Nodes do not neccesarily have parents of each type of label.\\n\\n    Your main focus should be to identify the Framework and the Function that is being asked.\\n    The question is:\\n    How to create a confusion matrix with Plotly?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > llm:ChatOpenAI] [754ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"MATCH (:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework {name: \\\"Plotly\\\"})-[:CONTAINS_FUNCTION]->(fn:Function {description: \\\"create a confusion matrix\\\"}) \\nRETURN f, fn;\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"MATCH (:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework {name: \\\"Plotly\\\"})-[:CONTAINS_FUNCTION]->(fn:Function {description: \\\"create a confusion matrix\\\"}) \\nRETURN f, fn;\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 46,\n",
      "                \"prompt_tokens\": 467,\n",
      "                \"total_tokens\": 513\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-effbe588-1560-40a5-acaa-b17c1c5be12f-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 467,\n",
      "              \"output_tokens\": 46,\n",
      "              \"total_tokens\": 513\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 46,\n",
      "      \"prompt_tokens\": 467,\n",
      "      \"total_tokens\": 513\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:run_cypher_query] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Generated query----> MATCH (:Area)-[:CONTAINS_FRAMEWORK]->(f:Framework {name: \"Plotly\"})-[:CONTAINS_FUNCTION]->(fn:Function {description: \"create a confusion matrix\"}) \n",
      "RETURN f, fn;\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence > chain:run_cypher_query] [36ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context> > chain:RunnableSequence] [808ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:RunnableParallel<graph_context,vector_context>] [823ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:context_unifier] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Graph context---> []\n",
      "Vector context---> [Document(metadata={'file_path': 'visualization\\\\plotly\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \"\"\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \"\"\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\'Blues\\',         showscale=True     )     fig.update_layout(title=\\'Confusion Matrix\\', xaxis_title=\\'Predicted Label\\', yaxis_title=\\'True Label\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\'%{text}\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\n\\nParameters:\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\n- class_names (list of str): List of class names corresponding to the labels.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\plotly\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \"\"\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \"\"\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\"all\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\"all\")      fig.update_layout(title=f\\'Violin Plot of {value_column} by {group_column}\\' if group_column else f\\'Violin Plot of {value_column}\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\n\\nParameters:\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\n- value_column (str): The column name of the DataFrame to be plotted.\\n- group_column (str, optional): The column name for grouping data into different violins.\\n\\nReturns:\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence > chain:context_unifier] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input> > chain:RunnableSequence] [1.41s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > chain:RunnableParallel<context,input>] [1.46s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou are a conversational chatbot, designed to answer based mainly in the context you are provided to the user questions.\\nQuestions are most likely related to coding doubts. You should provide code examples whenever possible.\\nContext: [Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\machine_learning_evaluation_plots.py', 'name': 'plot_confusion_matrix', 'code': 'def plot_confusion_matrix(confusion_matrix, class_names):     \\\"\\\"\\\"     Generates a confusion matrix plot from a sklearn confusion matrix object.      Parameters:     - confusion_matrix (array): Numpy array containing the confusion matrix information.     - class_names (list of str): List of class names corresponding to the labels.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.     \\\"\\\"\\\"     fig = ff.create_annotated_heatmap(         z=confusion_matrix,         x=class_names,         y=class_names,         colorscale=\\\\'Blues\\\\',         showscale=True     )     fig.update_layout(title=\\\\'Confusion Matrix\\\\', xaxis_title=\\\\'Predicted Label\\\\', yaxis_title=\\\\'True Label\\\\')     fig.update_traces(text=confusion_matrix.astype(str), texttemplate=\\\\'%{text}\\\\')     return fig'}, page_content='Generates a confusion matrix plot from a sklearn confusion matrix object.\\\\n\\\\nParameters:\\\\n- confusion_matrix (array): Numpy array containing the confusion matrix information.\\\\n- class_names (list of str): List of class names corresponding to the labels.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the confusion matrix plot.'), Document(metadata={'file_path': 'visualization\\\\\\\\plotly\\\\\\\\statistical_analysis_plots.py', 'name': 'violin_plot', 'code': 'def violin_plot(dataframe, value_column, group_column=None):     \\\"\\\"\\\"     Generates a violin plot for the specified column in a pandas DataFrame.      Parameters:     - dataframe (pd.DataFrame): The DataFrame containing the data.     - value_column (str): The column name of the DataFrame to be plotted.     - group_column (str, optional): The column name for grouping data into different violins.      Returns:     - fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.     \\\"\\\"\\\"     if group_column:         fig = px.violin(dataframe, y=value_column, x=group_column, box=True, points=\\\"all\\\")     else:         fig = px.violin(dataframe, y=value_column, box=True, points=\\\"all\\\")      fig.update_layout(title=f\\\\'Violin Plot of {value_column} by {group_column}\\\\' if group_column else f\\\\'Violin Plot of {value_column}\\\\')     return fig'}, page_content='Generates a violin plot for the specified column in a pandas DataFrame.\\\\n\\\\nParameters:\\\\n- dataframe (pd.DataFrame): The DataFrame containing the data.\\\\n- value_column (str): The column name of the DataFrame to be plotted.\\\\n- group_column (str, optional): The column name for grouping data into different violins.\\\\n\\\\nReturns:\\\\n- fig (plotly.graph_objects.Figure): The Plotly figure object for the violin plot.')]\\n\\nUser question: How to create a confusion matrix with it?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence > llm:ChatOpenAI] [2.92s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To create a confusion matrix using the provided function `plot_confusion_matrix` in the file `machine_learning_evaluation_plots.py`, you can follow these steps:\\n\\n1. Make sure you have a sklearn confusion matrix object and a list of class names ready.\\n2. Call the `plot_confusion_matrix` function with the confusion matrix and class names as parameters.\\n3. The function will generate a Plotly figure object representing the confusion matrix plot.\\n\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\n\\n```python\\nimport numpy as np\\nimport plotly.figure_factory as ff\\n\\n# Assuming you have a confusion matrix and class names\\nconfusion_matrix = np.array([[10, 2, 3], [1, 15, 2], [3, 1, 12]])\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Display the confusion matrix plot\\nfig.show()\\n```\\n\\nBy following these steps and providing the necessary inputs, you can create a confusion matrix plot using the `plot_confusion_matrix` function.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To create a confusion matrix using the provided function `plot_confusion_matrix` in the file `machine_learning_evaluation_plots.py`, you can follow these steps:\\n\\n1. Make sure you have a sklearn confusion matrix object and a list of class names ready.\\n2. Call the `plot_confusion_matrix` function with the confusion matrix and class names as parameters.\\n3. The function will generate a Plotly figure object representing the confusion matrix plot.\\n\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\n\\n```python\\nimport numpy as np\\nimport plotly.figure_factory as ff\\n\\n# Assuming you have a confusion matrix and class names\\nconfusion_matrix = np.array([[10, 2, 3], [1, 15, 2], [3, 1, 12]])\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Display the confusion matrix plot\\nfig.show()\\n```\\n\\nBy following these steps and providing the necessary inputs, you can create a confusion matrix plot using the `plot_confusion_matrix` function.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 242,\n",
      "                \"prompt_tokens\": 691,\n",
      "                \"total_tokens\": 933\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-790bc06f-a1d8-4ec0-9c07-3fb7d0f3458d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 691,\n",
      "              \"output_tokens\": 242,\n",
      "              \"total_tokens\": 933\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 242,\n",
      "      \"prompt_tokens\": 691,\n",
      "      \"total_tokens\": 933\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async > chain:RunnableSequence] [4.44s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory > chain:check_sync_or_async] [4.50s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableWithMessageHistory] [4.53s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "langchain.debug = True\n",
    "graph_retriever_chain =  cypher_gen_prompt | llm | RunnableLambda(run_cypher_query)\n",
    "\n",
    "# In this case we also add the rephrasing/summarising from the history:\n",
    "summarisation_chain =  {\"chat_history\": itemgetter('chat_history') | RunnableLambda(select_last_n_messages), \"input_message\": itemgetter('input')}| prompt_summarise_conver | llm | StrOutputParser()\n",
    "\n",
    "# Therefore the final chain will by:\n",
    "\n",
    "def context_unifier(full_context):\n",
    "    print(\"Graph context--->\",full_context['graph_context'])\n",
    "    print(\"Vector context--->\",full_context['vector_context'])\n",
    "    unified_context = full_context['graph_context'] + full_context['vector_context'] \n",
    "    return unified_context\n",
    "\n",
    "def get_schema(summarisation):\n",
    "    return graph.get_schema\n",
    "\n",
    "final_chain =  {'context' :summarisation_chain | {'graph_context': {'question': RunnablePassthrough(), 'schema': RunnableLambda(get_schema)} | graph_retriever_chain , 'vector_context':retriever} | RunnableLambda(context_unifier), 'input': itemgetter(\"input\")} | prompt_handle_conver | llm\n",
    "final_chain_history = RunnableWithMessageHistory(\n",
    "   final_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "\n",
    "output = final_chain_history.invoke({ \"input\": \"What is plotly?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    "\n",
    "output = final_chain_history.invoke({ \"input\": \"How to create a confusion matrix with it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To create a confusion matrix using the provided function `plot_confusion_matrix` in the file `machine_learning_evaluation_plots.py`, you can follow these steps:\\n\\n1. Make sure you have a sklearn confusion matrix object and a list of class names ready.\\n2. Call the `plot_confusion_matrix` function with the confusion matrix and class names as parameters.\\n3. The function will generate a Plotly figure object representing the confusion matrix plot.\\n\\nHere is an example of how you can use the `plot_confusion_matrix` function:\\n\\n```python\\nimport numpy as np\\nimport plotly.figure_factory as ff\\n\\n# Assuming you have a confusion matrix and class names\\nconfusion_matrix = np.array([[10, 2, 3], [1, 15, 2], [3, 1, 12]])\\nclass_names = ['Class A', 'Class B', 'Class C']\\n\\n# Call the plot_confusion_matrix function\\nfig = plot_confusion_matrix(confusion_matrix, class_names)\\n\\n# Display the confusion matrix plot\\nfig.show()\\n```\\n\\nBy following these steps and providing the necessary inputs, you can create a confusion matrix plot using the `plot_confusion_matrix` function.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 242, 'prompt_tokens': 691, 'total_tokens': 933}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-790bc06f-a1d8-4ec0-9c07-3fb7d0f3458d-0', usage_metadata={'input_tokens': 691, 'output_tokens': 242, 'total_tokens': 933})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > Next Steps<br><div>\n",
    "\n",
    "This is the initial development for the project ['Chat with your code'](https://github.com/hectorrrr/chat_with_your_code), but there are many potential improvements which we will address over a period of time. Here we list some of them:\n",
    "\n",
    "* Custom query 'Rewritting' for both Graph and Vector search\n",
    "* Test a 'Sequential' retriever instead of parallel, to not get duplicated data if unnecessary.\n",
    "* Create evaluation metrics to optimize the flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
